lr_decay_step: 100
gradients_log_interval: 100

optimizer: 'adam'
lr: 1e-3
momentum: 0.9
weight_decay: 0
checkpoint_path:

trainer:
  max_epochs: 100
  log_every_n_steps: 1
  check_val_every_n_epoch: 10
  accelerator: 'gpu'
