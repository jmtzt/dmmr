lr_decay_step: 100
gradients_log_interval: 100

optimizer: 'adam'
lr: 1e-4
momentum: 0.9
weight_decay: 0
checkpoint_path:

trainer:
  max_epochs: 500
  log_every_n_steps: 1
  check_val_every_n_epoch: 5
  accelerator: 'gpu'
